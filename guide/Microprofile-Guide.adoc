:projectid: kubernetes-caching-hazelcast-microprofile
:page-layout: guide
:page-duration: 15 minutes
:page-releasedate: 2019-06-20
:page-description: Explore how to use caching in microservices within Kubernetes environment.
:page-tags: ['Hazelcast', 'Caching', 'Microservices', 'Kubernetes', 'Containers','Microprofile' , 'Minikube']
:page-related-guides: ['docker', 'kubernetes-intro']
:source-highlighter: prettify
:page-seo-title: Caching in microservices with Hazelcast Tutorial
:page-seo-description: How to use Hazelcast with microservices
:templates-url: https://raw.githubusercontent.com/enozcan/adoc-templates/master

= Caching Microservices with Hazelcast in Kubernetes

Use Hazelcast Caching in Microprofile based Microservices and deploy to Kubernetes

:minikube-ip: 192.168.99.100
:project-id: kubernetes-caching-hazelcast-microprofile
:kube: Kubernetes
:hashtag: #
:win: WINDOWS
:mac: MAC
:linux: LINUX
:hazelcast: Hazelcast
:framework: Microprofile
:framework-short: microprofile
:github-address: https://github.com/enozcan/guides-kubernetes-caching-hazelcast-microprofile

include::{templates-url}/what-will-learn.adoc[]

include::{templates-url}/what-is-hz.adoc[]

== Why Microprofile?

The MicroProfile is a baseline platform definition that optimizes Enterprise Java for a microservices architecture and delivers application portability across multiple MicroProfile runtimes.
To learn more about Microprofile. 
https://microprofile.io/

include::{templates-url}/prereq.adoc[]

include::{templates-url}/getting-started.adoc[]

== Running Microprofile Application

The application in initial directory is a basic Microprofile app having a few endpoints. We are going to use only */application/map/put* and */application/map/get* endpoints through this guide.

Build and run the app using Maven in the `initial` directory:

----
$ > mvn install liberty:run-server 
----

When the log  `"The GettingStartedServer server is ready to run a smarter planet."` is seen, the app is ready and runnning on localhost:9080. You can test by following requests:

----
$ > curl "http://localhost:9080/application/map/put?key=key_1&value=hazelcast"
$ > curl "http://localhost:9080/application/map/get?key=key_1"
----
The first one will not return a response. The second one will return the value belongs to the key given as parameter(`key_1` and `hazelcast` in this request) and the responding pod (currently `null`).

This part was an introduction of the applicaiton. You can stop your application by *CTRL + C*.

== Dockerizing the App

To create the docker image of the application, use `docker-image` profile existing in the `pom.xml`.
This profile will build the docker image using the Dockerfile under `initial` directory.


Build the app under `initial` directory using profile:
----
$ > mvn clean package -P docker-image
----

Now, the image must be seen among the Docker images:
----
$ > docker images

REPOSITORY                           TAG             IMAGE ID        CREATED             SIZE
openliberty-hazelcast-microprofile   1.0-SNAPSHOT    275a0f74c8ba    27 seconds ago      452MB
----

== Running the app in container

Now that the Docker image is ready, check if the image runs properly:

----
$ > docker run -p 9080:9080 openliberty-hazelcast-microprofile:1.0-SNAPSHOT
----

Test the app on the port 9080:
----
$ > curl "http://localhost:9080/application/map/put?key=key_1&value=hazelcast"
$ > curl "http://localhost:9080/application/map/get?key=key_1"
----
If you see the same responses as the ones you get when the app is run without container, that means it's all OK with the image.

To stop the container, get the container ID first:
----
$ > docker ps
----
Then find the application's container ID and stop the container:
----
$ > docker stop [CONTAINER-ID]
----

include::{templates-url}/start-cluster.adoc[]

include::{templates-url}/validate-kube-env.adoc[]


After you are sure that a master node is ready, create kubernetes.yaml under `initial` directory with the same content in the `final/kubernetes.yaml` file.

This file defines two {kube} resources: one statefulset and one service. 
StatefulSet is preferred solution for Hazelcast because it enables controlled scale out/in of your microservices 
for easy data distribution. To learn more about StatefulSet, you can visit Kubernetes documentation: 
https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/

By default, we create 2 replicas of `hazelcast-microprofile` microservice behind the `hazelcast-microprofile-service` which forwards requests to one of the pods available in the kubernetes cluster.

`MY_POD_NAME` is an environment variable made available to the pods so that each microservice knows which pod they are in.
This is going to be used in this guide in order to show which pod is responding to the http request. It's fetched and used in the `MapResource.java` file.

Now, run the following command to deploy the resources as defined in kubernetes.yaml:
----
$ > kubectl apply -f kubernetes.yaml
----
Run the following command to check the status of your pods:
----
$ > kubectl get pods
----

You'll see an output similar to the following if all the pods are healthy and running:
----
NAME                                   READY     STATUS    RESTARTS   AGE
hazelcast-microprofile-statefulset-0   1/1       Running   0          7s
hazelcast-microprofile-statefulset-1   1/1       Running   0          3s
----
Even if the status of the nodes are `Running`, they might be not started yet. Check the pod logs to be sure they're ready:
----
$ > kubectl logs hazelcast-microprofile-statefulset-0
----
You must see those lines among other log reports:
----
SSL certificate created in 58.745 seconds.
The defaultServer server is ready to run a smarter planet..
----

And add a value to the map and then get the value:
----

$ > curl "http://localhost:31000/application/map/put?key=key1&value=hazelcast"
$ > while true; do curl localhost:31000/application/map/get?key=key1;echo; sleep 2; done

hazelcast from hazelcast-microprofile-statefulset-0
hazelcast from hazelcast-microprofile-statefulset-0
null from hazelcast-microprofile-statefulset-1
null from hazelcast-microprofile-statefulset-1
----

As can be seen, data is inserted by `hazelcast-microprofile-statefulset-0` and  not shared with the other node. Here is where Hazelcast comes into action.

Before going in to the next step, kill active pods under `initial` directory by:
----
$ > kubectl delete -f kubernetes.yaml
----

== Hazelcast Caching among Kubernetes pods

Now we will use Hazelcast Caching among the pods. Update the pom.xml file by adding those dependencies under the line  *`<!-- Hazelcast Dependencies -->`* :
----
<dependency>
    <groupId>com.hazelcast</groupId>
    <artifactId>hazelcast</artifactId>
    <version>${version.hazelcast}</version>
</dependency>

<dependency>
    <groupId>com.hazelcast</groupId>
    <artifactId>hazelcast-kubernetes</artifactId>
    <version>${version.hazelcast-kubernetes}</version>
</dependency>
----

Modify `MapApplicaiton.java` and create an application scoped Hazelcast instance inside the class:
----
...
...
import com.hazelcast.config.Config;
import com.hazelcast.config.JoinConfig;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import javax.enterprise.inject.Produces;

public class MapApplication extends Application {
    @Produces
    HazelcastInstance create() {
        Config config = new Config();
        config.getGroupConfig().setName("MP-GUIDE");
        JoinConfig joinConfig = config.getNetworkConfig().getJoin();
        joinConfig.getMulticastConfig().setEnabled(false);
        joinConfig.getKubernetesConfig().setEnabled(true);
        return Hazelcast.newHazelcastInstance(config);
    }
}
----

Then modify `MapManager.java` such that map is fetched from Hazelcast instance:
----
...
...
import javax.inject.Inject;
import com.hazelcast.core.HazelcastInstance;

@ApplicationScoped
public class MapManager {

    @Inject
    HazelcastInstance instance;

    //Map<String,String> keyValueStore = new ConcurrentHashMap<>();

    private Map<String,String> retrieveMap() {
        return instance.getMap("map");
    }

    ...
    ...
}
----

Before deploying on kubernetes, create rbac.yaml file as in the `final` directory. Role Based Access Controller(RBAC) configuration is used to give access to Kubernetes Master API from pods which runs microservices. Hazelcast requires a read access to autodiscover other hazelcast members and form hazelcast cluster.

*rbac.yaml:*
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: default-cluster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
  - kind: ServiceAccount
    name: default
    namespace: default
----

Rebuild the app and create new image:
----
$ > mvn clean package -P docker-image
----

Run the following commands to deploy the resources as defined in kubernetes.yaml and rbac.yaml in the specified order:
----
$ > kubectl apply -f rbac.yaml
----
----
$ > kubectl apply -f kubernetes.yaml
----

Run the following command to check the status of your pods:
----
$ > kubectl get pods
----

Even if the status of the nodes are `Running`, they might be not started yet. Check the pod logs to be sure they're ready:
----
$ > kubectl logs hazelcast-microprofile-statefulset-0
$ > kubectl logs hazelcast-microprofile-statefulset-1
----
You must see those lines among other log reports. If not, wait for servers to be start:
----
SSL certificate created in 58.745 seconds.
The defaultServer server is ready to run a smarter planet..
----

Now we expect all nodes to give the same value for the same key put on the map by a particular pod. Let's try:
----
$ > curl "http://localhost:31000/application/map/put?key=key_1&value=hazelcast"
$ > while true; do curl localhost:31000/application/map/get?key=key_1;echo; sleep 2; done

hazelcast from hazelcast-microprofile-statefulset-1
hazelcast from hazelcast-microprofile-statefulset-0
hazelcast from hazelcast-microprofile-statefulset-0
hazelcast from hazelcast-microprofile-statefulset-1
----

As can be seen both nodes give the same value for the key now.

== Scaling with Hazelcast

Scale the cluster with one more pod and see that you still retrieve the shared data.

[role='command']
```
$ > kubectl scale statefulset hazelcast-microprofile-statefulset --replicas=3
```

Run following command to see the latest status of the pods
[role='command']
```
$ > kubectl get pods
```

As you can see, a new pod `hazelcast-microprofile-statefulset-2` has joined to the cluster.
[source, role="no_copy"]
----
NAME                                   READY     STATUS    RESTARTS   AGE
hazelcast-microprofile-statefulset-0   1/1       Running   0          13m
hazelcast-microprofile-statefulset-1   1/1       Running   0          13m
hazelcast-microprofile-statefulset-2   1/1       Running   0          6s
----

Wait for new pod to start and then run the following command again to see the output:
[role='command']
```
$ > while true; do curl "http://localhost:31000/application/map/get?key=key_1";echo; sleep 2; done
```
----
hazelcast from hazelcast-microprofile-statefulset-1
hazelcast from hazelcast-microprofile-statefulset-2
hazelcast from hazelcast-microprofile-statefulset-0
hazelcast from hazelcast-microprofile-statefulset-2
----

As you can see, `hazelcast-microprofile-statefulset-2` is returning correct data.


== Testing microservices that are running on {kube}

Create a testing class under `/initial/src/test/java/io/openliberty/sample/system` named `MapResourceTest.java` .The contents of the test file is available under `final` directoy.

Also change the pom.xml file simply by uncommenting the testing dependencies & plugins.

The test makes sure that the */put* endpoint is handled by one pod and */get* methods returns the same data from the other kubernetes pod.

It first puts a key/value pair to hazelcast-microprofile microservice and keeps podname in the firstpod variable. In the second part, tests submits multiple */get* requests until to see that podname is different than the pod which initially handled */put* request.

In order to run integration tests, you must have a running hazelcast-microprofile microservices in minikube environment. As you have gone through all previous steps, you already have it.

Run test under `initial` directory:

----
$ > mvn -Dtest=MapResourceTest test
----

If the tests pass, you’ll see a similar output to the following:

----
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running io.openliberty.sample.system.MapResourceTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.48 s - in io.openliberty.sample.system.MapResourceTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  19.801 s
[INFO] Finished at: 2019-06-26T17:40:50+03:00
[INFO] ------------------------------------------------------------------------
----


include::{templates-url}/minikube-teardown.adoc[]

include::{templates-url}/youre-done.adoc[]
